{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In cases, for high dimensional feature space or if the feature space is very sparse, Support Vector Machine (SVM) would be good candidate. In such cases, SVM has much better performance than Random Forest.\n",
    "2. SVM is also robust to overfitting because it contains the epsilon parameter which acts as regularizer thereby preventing overfitting. Epsilon parameter can be fine-tuned to get optimum model performance on test data. \n",
    "3. As SVM uses hinge loss to get the optimum hyperplane separating the two classes, it is robust to outlier presence. The soft margin-based hyperplane depends solely on few points known as support vectors which are close to the hyperplane. Other points don't have much of an impact on the optimum hyperplane. \n",
    "4. We can account for correlation between input features by adding regularization term to the hinge loss. Adding regularization term, the weights of variables which are artificially boosted as they are related to another variable which strongly impacts the outcome variable.\n",
    "5. Although non-linear decision boundaries are not modelled accurately by SVM, we can use kernel transformation of features to higher dimensional however it is not possible to get feature importance as the features are already transformed due to kernel transformation.\n",
    "6. SVM is sensitive to variable scale and it gives more weightage variables of higher magnitude. However, we can overcome this by first re-scaling all the variables, so they have the same range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dgxuser_layersvanguard/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/dgxuser_layersvanguard/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from stemmer_util import Stem\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, GridSearchCV\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = pd.read_csv(\"train_set.csv\", engine ='python')\n",
    "tweet_data[['has_hashtag','has_mentions','has_url','has_media']] = tweet_data[['has_hashtag','has_mentions','has_url','has_media']].astype(bool)\n",
    "all_stopwords = stopwords.words('english') + stopwords.words('german') + stopwords.words('italian') + stopwords.words('french') + stopwords.words('portuguese') + stopwords.words('spanish')\n",
    "all_stopwords.extend(['http', 'https'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgxuser_layersvanguard/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    " # Preprocessing\n",
    "#1. Removing rows with rare/undefine authors\n",
    "tweet_data = tweet_data[tweet_data['author'].isin(['Barack Obama', 'Kim Kardashian West', 'KATY PERRY', 'Snoop Dogg', 'Cristiano Ronaldo', 'Elon Musk', 'Ellen DeGeneres', 'Sebastian Ruder', 'Donald J. Trump'])].reset_index(drop=True)\n",
    "#2. convert to lower case and  removing punctuation\n",
    "tweet_data['tweet'] = tweet_data['tweet'].str.lower().str.replace('[^\\w\\s]', '').apply(lambda x: \" \".join(x.split()))\n",
    "tweet_data['tweet'] = tweet_data['tweet'].apply(lambda x: \" \".join([word for word in x.split() if word != \"\"]))\n",
    "tweet_data.loc[~tweet_data['lang'].isin(['en', 'und', 'es', 'pt']),'lang'] = 'others'\n",
    "source_map = {}\n",
    "with open(\"source_mapping.txt\" ,'r') as inp:\n",
    "    for line in inp:\n",
    "        source_map[int(line.strip().split(\":\")[1].strip())] = line.strip().split(\":\")[0]\n",
    "\n",
    "tweet_data['source_info'] = tweet_data['source'].apply(lambda x:source_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_encoder = OneHotEncoder().fit(tweet_data[['source_info']])\n",
    "source_enc_df = pd.DataFrame(source_encoder.transform(tweet_data[['source_info']]).todense())\n",
    "source_enc_df.columns = source_encoder.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = pd.concat([tweet_data[['author','lang','has_hashtag','has_mentions','has_url','has_media','tweet']],source_enc_df], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweet_data.drop(columns = ['author'])\n",
    "output_enc = LabelEncoder().fit(tweet_data['author'])\n",
    "Y = output_enc.transform(tweet_data['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feature_ext_ppl = Pipeline([('stem',Stem(do_stem=False, str_col = 'tweet', lang_col='lang')), \n",
    "                                 ('tfidf',ColumnTransformer(transformers=[('tfidf', TfidfVectorizer(stop_words=all_stopwords, analyzer='word'), \n",
    "                                'tweet')], remainder = 'passthrough', sparse_threshold=0))])\n",
    "Feature_ext_pipeline = Pipeline([('text_feat_ext', text_feature_ext_ppl),\n",
    "                                 ('feature_scaling', RobustScaler()), \n",
    "                                 ('Kernel_trans',Nystroem(n_components=1000))])\n",
    "model_pipeline = Pipeline([(\"feature_ext\",Feature_ext_pipeline ), ('svm', SGDClassifier(loss='hinge', penalty='elasticnet', random_state=100, verbose=1, early_stopping=True, max_iter=1000, tol=0.001))])\n",
    "\n",
    "hyperparameters = {'feature_ext__text_feat_ext__stem__do_stem':(True,False),\n",
    "                    'feature_ext__Kernel_trans__kernel': ('linear','rbf', 'poly', 'sigmoid'), \n",
    "                   'feature_ext__Kernel_trans__gamma':(0.001, 0.01, 0.1), \n",
    "                   'svm__alpha': (0.01, 0.1, 0.5, 1, 2, 5), \n",
    "                   'svm__l1_ratio': list(np.arange(0, 1.1, 0.2))}\n",
    "opt_svm = RandomizedSearchCV(model_pipeline, hyperparameters,scoring='f1_micro', n_jobs=12, cv=5, verbose=True, error_score=np.nan, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training support vector machine classifier ...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed: 68.3min\n",
      "[Parallel(n_jobs=12)]: Done 500 out of 500 | elapsed: 79.5min finished\n",
      "/home/dgxuser_layersvanguard/meesh/stemmer_util.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.str_col] = l\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.38, NNZs: 1000, Bias: -1.109418, T: 14823, Avg. loss: 0.217448\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.36, NNZs: 1000, Bias: -1.074434, T: 29646, Avg. loss: 0.211259\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.35, NNZs: 1000, Bias: -1.058904, T: 44469, Avg. loss: 0.210916\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.35, NNZs: 1000, Bias: -1.048198, T: 59292, Avg. loss: 0.210831\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.34, NNZs: 1000, Bias: -1.055650, T: 74115, Avg. loss: 0.210869\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.34, NNZs: 1000, Bias: -1.035724, T: 88938, Avg. loss: 0.210843\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.34, NNZs: 1000, Bias: -1.047302, T: 103761, Avg. loss: 0.210829\n",
      "Total training time: 0.89 seconds.\n",
      "Convergence after 7 epochs took 0.90 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.08, NNZs: 1000, Bias: -0.725183, T: 14823, Avg. loss: 0.186082\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.05, NNZs: 1000, Bias: -0.722759, T: 29646, Avg. loss: 0.179485\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.05, NNZs: 1000, Bias: -0.723253, T: 44469, Avg. loss: 0.179289\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.05, NNZs: 1000, Bias: -0.722681, T: 59292, Avg. loss: 0.179164\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.05, NNZs: 1000, Bias: -0.725096, T: 74115, Avg. loss: 0.179104\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.05, NNZs: 1000, Bias: -0.721101, T: 88938, Avg. loss: 0.179128\n",
      "Total training time: 0.78 seconds.\n",
      "Convergence after 6 epochs took 0.79 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.20, NNZs: 1000, Bias: -1.190477, T: 14823, Avg. loss: 0.015857\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.14, NNZs: 1000, Bias: -1.127576, T: 29646, Avg. loss: 0.014536\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.11, NNZs: 1000, Bias: -1.097463, T: 44469, Avg. loss: 0.014494\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.10, NNZs: 1000, Bias: -1.078167, T: 59292, Avg. loss: 0.014476\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.09, NNZs: 1000, Bias: -1.066115, T: 74115, Avg. loss: 0.014465\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.08, NNZs: 1000, Bias: -1.056344, T: 88938, Avg. loss: 0.014458\n",
      "Total training time: 0.73 seconds.\n",
      "Convergence after 6 epochs took 0.74 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.47, NNZs: 1000, Bias: -0.783880, T: 14823, Avg. loss: 0.147701\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.49, NNZs: 1000, Bias: -0.757925, T: 29646, Avg. loss: 0.140676\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.50, NNZs: 1000, Bias: -0.758388, T: 44469, Avg. loss: 0.140168\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.50, NNZs: 1000, Bias: -0.758662, T: 59292, Avg. loss: 0.140019\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.49, NNZs: 1000, Bias: -0.754690, T: 74115, Avg. loss: 0.140023\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.50, NNZs: 1000, Bias: -0.759999, T: 88938, Avg. loss: 0.140216\n",
      "Total training time: 0.77 seconds.\n",
      "Convergence after 6 epochs took 0.79 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.88, NNZs: 1000, Bias: -1.158762, T: 14823, Avg. loss: 0.223086\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.85, NNZs: 1000, Bias: -1.101164, T: 29646, Avg. loss: 0.216489\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.83, NNZs: 1000, Bias: -1.095220, T: 44469, Avg. loss: 0.216187\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.83, NNZs: 1000, Bias: -1.071793, T: 59292, Avg. loss: 0.216081\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.82, NNZs: 1000, Bias: -1.074841, T: 74115, Avg. loss: 0.216074\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.82, NNZs: 1000, Bias: -1.064549, T: 88938, Avg. loss: 0.216049\n",
      "Total training time: 0.83 seconds.\n",
      "Convergence after 6 epochs took 0.85 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.41, NNZs: 1000, Bias: -0.947875, T: 14823, Avg. loss: 0.215953\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.39, NNZs: 1000, Bias: -0.947428, T: 29646, Avg. loss: 0.207360\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.39, NNZs: 1000, Bias: -0.949973, T: 44469, Avg. loss: 0.207162\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.38, NNZs: 1000, Bias: -0.944401, T: 59292, Avg. loss: 0.207050\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.38, NNZs: 1000, Bias: -0.945143, T: 74115, Avg. loss: 0.206927\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.38, NNZs: 1000, Bias: -0.944526, T: 88938, Avg. loss: 0.207024\n",
      "Total training time: 0.78 seconds.\n",
      "Convergence after 6 epochs took 0.79 seconds\n",
      "-- Epoch 1\n",
      "Norm: 0.87, NNZs: 1000, Bias: -1.061150, T: 14823, Avg. loss: 0.227359\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.85, NNZs: 1000, Bias: -1.049509, T: 29646, Avg. loss: 0.219952\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.83, NNZs: 1000, Bias: -1.043640, T: 44469, Avg. loss: 0.219738\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.83, NNZs: 1000, Bias: -1.039965, T: 59292, Avg. loss: 0.219606\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.82, NNZs: 1000, Bias: -1.041610, T: 74115, Avg. loss: 0.219553\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.82, NNZs: 1000, Bias: -1.038670, T: 88938, Avg. loss: 0.219540\n",
      "Total training time: 0.77 seconds.\n",
      "Convergence after 6 epochs took 0.79 seconds\n",
      "-- Epoch 1\n",
      "Norm: 2.62, NNZs: 1000, Bias: -0.680291, T: 14823, Avg. loss: 0.041519\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.61, NNZs: 1000, Bias: -0.676522, T: 29646, Avg. loss: 0.038415\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.60, NNZs: 1000, Bias: -0.680851, T: 44469, Avg. loss: 0.038248\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.60, NNZs: 1000, Bias: -0.681481, T: 59292, Avg. loss: 0.038197\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.60, NNZs: 1000, Bias: -0.684169, T: 74115, Avg. loss: 0.038193\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.60, NNZs: 1000, Bias: -0.682559, T: 88938, Avg. loss: 0.038170\n",
      "Total training time: 0.75 seconds.\n",
      "Convergence after 6 epochs took 0.77 seconds\n",
      "-- Epoch 1\n",
      "Norm: 1.99, NNZs: 1000, Bias: -0.870589, T: 14823, Avg. loss: 0.112336\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.98, NNZs: 1000, Bias: -0.871690, T: 29646, Avg. loss: 0.108389\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.98, NNZs: 1000, Bias: -0.871775, T: 44469, Avg. loss: 0.108229\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.98, NNZs: 1000, Bias: -0.869834, T: 59292, Avg. loss: 0.108190\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.98, NNZs: 1000, Bias: -0.871184, T: 74115, Avg. loss: 0.108171\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.98, NNZs: 1000, Bias: -0.872240, T: 88938, Avg. loss: 0.108153\n",
      "Total training time: 0.75 seconds.\n",
      "Convergence after 6 epochs took 0.77 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    7.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=100)\n",
    "print('\\nTraining support vector machine classifier ...')\n",
    "opt_svm.fit(X_train, Y_train)\n",
    "joblib.dump(opt_svm.best_estimator_, 'svm_model.pkl', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7338352255479327"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_svm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svm__l1_ratio': 0.0,\n",
       " 'svm__alpha': 0.01,\n",
       " 'feature_ext__text_feat_ext__stem__do_stem': True,\n",
       " 'feature_ext__Kernel_trans__kernel': 'linear',\n",
       " 'feature_ext__Kernel_trans__gamma': 0.001}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgxuser_layersvanguard/meesh/stemmer_util.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[self.str_col] = l\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.754735308402137"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_test\n",
    "metrics.f1_score(Y_test, opt_svm.predict(X_test), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['source_encoder.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(output_enc, \"output_encoder.pkl\")\n",
    "joblib.dump(source_encoder, \"source_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
